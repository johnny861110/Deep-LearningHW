{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73ed2f36",
   "metadata": {},
   "source": [
    "### Tuning Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9fd4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 08m 29s]\n",
      "val_accuracy: 0.8539999723434448\n",
      "\n",
      "Best val_accuracy So Far: 0.9227222204208374\n",
      "Total elapsed time: 00h 26m 36s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "{'activation_fn': 'tanh', 'hidden_node': 8, 'init_weight': 'glorot_uniform', 'optimizer_type': 'SGD', 'regularization_coefficient': 0.001, 'learning_rate': 0.09910000000000001, 'learning_decay_schedule': 'cosine', 'loss_function': 'categorical_crossentropy', 'regularization_coefficients': 0.001, 'decay_schedule': 'cosine', 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from model_bulid import build_model\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "from kerastuner import HyperParameters\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot encode the target variable\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# validation dataset\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.3)\n",
    "\n",
    "# Define the search space\n",
    "hp = HyperParameters()\n",
    "hp.Choice('activation_fn', ['tanh', 'relu'])\n",
    "hp.Int('hidden_node', 5, 11, 1)\n",
    "hp.Choice('init_weight', ['random_normal', 'glorot_uniform', 'he_uniform'])\n",
    "hp.Choice('optimizer_type', ['SGD', 'Adam'])\n",
    "hp.Float('regularization_coefficient', 0.0001, 0.001,step=0.0009)\n",
    "hp.Float('learning_rate', 0.0001, 0.1,step=0.099)\n",
    "hp.Choice('learning_decay_schedule', ['none', 'cosine'])\n",
    "hp.Choice('loss_function', ['categorical_crossentropy', 'mean_squared_error'])\n",
    "\n",
    "# Define the tuner\n",
    "tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    hyperparameters=hp,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=3,\n",
    "    num_initial_points=2,\n",
    "    overwrite=True,\n",
    "    directory='bayesian_opt',\n",
    "    project_name='mnist'\n",
    ")\n",
    "\n",
    "# Define a function to print the details of each trial\n",
    "def on_trial_end(trial):\n",
    "    print(f'Trial {trial.trial_id}:\\n')\n",
    "    print(f'Hyperparameters: {trial.hyperparameters}\\n')\n",
    "    print(f'Metrics: {trial.metrics}\\n')\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=hp.Int('epochs',100,300,step=100),\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[keras.callbacks.LambdaCallback(on_trial_end=on_trial_end)]\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(tuner.get_best_hyperparameters()[0].values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dbcab70",
   "metadata": {},
   "source": [
    "### see performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6e380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3107 - accuracy: 0.9240\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3842 - accuracy: 0.8952\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5452 - accuracy: 0.8645\n",
      "top model Test accuracy:0.9240000247955322\n",
      "top model loss : 0.31067758798599243\n",
      "\n",
      "\n",
      "second model Test accuracy:0.8952000141143799\n",
      "second model loss : 0.3842472732067108\n",
      "\n",
      "\n",
      "third model Test accuracy:0.8644999861717224\n",
      "third model loss : 0.5452260375022888\n",
      "\n",
      "\n",
      "ensemble_mean：0.8945666750272115\n"
     ]
    }
   ],
   "source": [
    "top_models = tuner.get_best_models(num_models=3)\n",
    "top_parameters = tuner.get_best_hyperparameters(num_trials=3)\n",
    "models = list(map(lambda x: x ,top_models))\n",
    "top = models[0]\n",
    "second = models[1]\n",
    "third = models[2]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_top, accuracy_top =top.evaluate(X_test, y_test)\n",
    "loss_second, accuracy_second =second.evaluate(X_test, y_test)\n",
    "loss_third, accuracy_third =third.evaluate(X_test, y_test)\n",
    "print(f'top model Test accuracy:{accuracy_top}')\n",
    "print(f'top model loss : {loss_top}')\n",
    "print('\\n')\n",
    "print(f'second model Test accuracy:{accuracy_second}')\n",
    "print(f'second model loss : {loss_second}')\n",
    "print('\\n')\n",
    "print(f'third model Test accuracy:{accuracy_third}')\n",
    "print(f'third model loss : {loss_third}')\n",
    "print('\\n')\n",
    "ensemble_mean = (accuracy_top + accuracy_second + accuracy_third)/3\n",
    "print(f'ensemble_mean：{ensemble_mean}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8073c8ca",
   "metadata": {},
   "source": [
    "### Save model (weight&model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28ddfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 2_layer_top_NNmodel.tf\\assets\n",
      "INFO:tensorflow:Assets written to: 2_layer_second_NNmodel.tf\\assets\n",
      "INFO:tensorflow:Assets written to: 2_layer_third_NNmodel.tf\\assets\n"
     ]
    }
   ],
   "source": [
    "top.save('2_layer_top_NNmodel.tf')\n",
    "top.save_weights('2_layer_top_NNmodel_weight.tf')\n",
    "second.save('2_layer_second_NNmodel.tf')\n",
    "second.save_weights('2_layer_second_NNmodel_weight.tf')\n",
    "third.save('2_layer_third_NNmodel.tf')\n",
    "third.save_weights('2_layer_third_NNmodel_weight.tf')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53f1699b",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2.3_py_3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
